{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 10\n",
    "BATCH_SIZE = 1\n",
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_SIZE = 64\n",
    "ENGLISH_VOCAB_SIZE = 25\n",
    "FRENCH_VOCAB_SIZE = 25\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import get_data, decode\n",
    "dataloader = get_data(MAX_SEQ_LENGTH, False, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "\tdef __init__(self, input_size, hidden_size, num_layers=1):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tself.embedding = torch.nn.Embedding(ENGLISH_VOCAB_SIZE, EMBEDDING_SIZE, dtype=torch.float32)\n",
    "\t\tself.rnn = torch.nn.RNN(EMBEDDING_SIZE, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tembedding = self.embedding(x)\n",
    "\t\toutput, hn = self.rnn(embedding)\n",
    "\t\treturn output, hn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alignment Model\n",
    "Although it can be done by concatenating both the annotation and prev hidden state, and making a single layered neural network, keeping them seperate helps understand why \"weighting\" said inputs are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignmentModel(torch.nn.Module):\n",
    "\tdef __init__(self, hidden_size):\n",
    "\t\tsuper(AlignmentModel, self).__init__()\n",
    "\t\tself.W = torch.nn.Linear(hidden_size, hidden_size)\n",
    "\t\tself.U = torch.nn.Linear(hidden_size*2, hidden_size)\n",
    "\t\tself.V = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "\tdef forward(self, encoder_annotations, decoder_prev_hidden): # query, keys\n",
    "\t\tdecoder_prev_hidden = decoder_prev_hidden\n",
    "\t\toutput = self.W(decoder_prev_hidden.permute(1,0,2)) + self.U(encoder_annotations) # check notes \n",
    "\t\toutput = torch.tanh(output)\n",
    "\t\tscores = self.V(output) # batch, sequence, score (1)\n",
    "\t\tscores = scores.permute(0,2,1) # batch, score, sequence bc annotations is batch, sequence (1), hidden*2\n",
    "\t\tweights = torch.softmax(scores, dim=2)\n",
    "\t\tcontext = torch.bmm(weights, encoder_annotations)\n",
    "\t\treturn context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "\tdef __init__(self, input_size, hidden_size, num_layers=1):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\t\tself.embedding = torch.nn.Embedding(FRENCH_VOCAB_SIZE, EMBEDDING_SIZE, dtype=torch.float32)\n",
    "\t\tself.rnn = torch.nn.RNN(EMBEDDING_SIZE+(2*hidden_size), hidden_size, num_layers, bidirectional=False, batch_first=True)\n",
    "\t\tself.linear = torch.nn.Linear(hidden_size, FRENCH_VOCAB_SIZE)\n",
    "\t\tself.alignment_model = AlignmentModel(hidden_size)\n",
    "\n",
    "\tdef forward(self, encoder_outputs, encoder_hiddens):\n",
    "\t\tdecoder_outputs = []\n",
    "\t\tdecoder_hidden = encoder_hiddens[1].unsqueeze(0) # final hidden state of backwards\n",
    "\t\tdecoder_input = torch.empty(BATCH_SIZE, 1, 1, dtype=torch.long).fill_(1).to(device) # batch size, seq len = 1, <EOS> TOKEN (using embedding layer so input is an integer)\n",
    "\t\tfor t in range(MAX_SEQ_LENGTH):\n",
    "\t\t\tdecoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\t\t\tdecoder_outputs.append(decoder_output)\n",
    "\t\t\t_, decoder_output = decoder_output.detach().topk(1, dim=2)\n",
    "\t\t\tdecoder_input = decoder_output\n",
    "\t\t\n",
    "\t\tdecoder_outputs = torch.concat(decoder_outputs, dim=1).squeeze(-1) # batch, seq len, token\n",
    "\n",
    "\t\treturn decoder_outputs, decoder_hidden\n",
    "\t\n",
    "\tdef forward_step(self, x, hn, encoder_annotations): # write this code\n",
    "\t\tembedding = self.embedding(x).squeeze(1)\n",
    "\t\tcontext = self.alignment_model(encoder_annotations, hn)\n",
    "\t\tdecoder_input = torch.concat((embedding, context), dim=2) # batch, seq len = 1, both embedding and context\n",
    "\t\tdecoder_output, decoder_hn = self.rnn(decoder_input, hn)\n",
    "\t\tdecoder_output = self.linear(decoder_output)\n",
    "\t\tdecoder_output = torch.softmax(decoder_output, dim=2) # batch, seq, pred\n",
    "\t\treturn decoder_output, decoder_hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(torch.nn.Module):\n",
    "\tdef __init__(self, encoder, decoder):\n",
    "\t\tsuper(EncoderDecoder, self).__init__()\n",
    "\t\tself.encoder = encoder\n",
    "\t\tself.decoder = decoder\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tencoder_outputs, encoder_hiddens = self.encoder(x)\n",
    "\t\tdecoder_outputs, decoder_hiddens = self.decoder(encoder_outputs, encoder_hiddens)\n",
    "\t\treturn decoder_outputs, decoder_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder(Encoder(1,128), Decoder(1,128)).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2.2854373455047607\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(750):\n",
    "\n",
    "\trunning_loss = 0.0\n",
    "\tfor i, data in enumerate(dataloader):\n",
    "\t\tinputs, labels = data\n",
    "\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\toutputs, hn = model(inputs)\n",
    "\t\tloss = loss_fn(outputs.squeeze(0), labels.squeeze(0))\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\n",
    "\tprint(f'\\r {running_loss/len(dataloader)}', end='', flush=True)\n",
    "\n",
    "print('\\nFinished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> i like to eat fish <EOS> <PAD> <PAD> <PAD>\n",
      "<SOS> toi thit an ca <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "<SOS> toi thit an ca <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "<SOS> have you ate yet <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "<SOS> co an com chua <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "<SOS> co an com chua <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "<SOS> we are going to church tomorrow <EOS> <PAD> <PAD>\n",
      "<SOS> ngay may minh di le <EOS> <PAD> <PAD> <PAD>\n",
      "<SOS> ngay may minh di le <EOS> <PAD> <PAD> <PAD>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\tfor i, data in enumerate(dataloader):\n",
    "\t\tinputs, labels = data\n",
    "\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
    "\t\toutputs, hn = model(inputs)\n",
    "\t\tprint(decode(\"english\", inputs.squeeze().tolist()))\n",
    "\t\tprint(decode(\"vietnamese\", labels.squeeze().tolist()))\n",
    "\t\t_, outputs = outputs.detach().topk(1, dim=2)\n",
    "\t\tprint(decode(\"vietnamese\", outputs.squeeze().tolist()))\n",
    "\t\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to account for init input for decoder\n",
    "# have to control the index value for special tokens\n",
    "# need to reverse labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes for cross entropy loss and data that has idx as target\n",
    "# output should be the softmax output vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
