{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# implement a rnn encoder decoder network for seq 2 seq\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<SOS> i like to eat fish <EOS> <PAD> <PAD> <PAD>', '<SOS> toi thit an ca <EOS> <PAD> <PAD> <PAD> <PAD>'], ['<SOS> have you ate yet <EOS> <PAD> <PAD> <PAD> <PAD>', '<SOS> co an com chua <EOS> <PAD> <PAD> <PAD> <PAD>'], ['<SOS> we are going to church tomorrow <EOS> <PAD> <PAD>', '<SOS> ngay may minh di le <EOS> <PAD> <PAD> <PAD>']]\n"
     ]
    }
   ],
   "source": [
    "# to keep thing simple lets use one hot encoding representations for our sequential data\n",
    "# we are going to split the data X, Y\n",
    "data = [[\"i like to eat fish\", \"toi thit an ca\"],\n",
    "\t\t[\"have you ate yet\", \"co an com chua\"],\n",
    "\t\t[\"we are going to church tomorrow\", \"ngay may minh di le\"]]\n",
    "\n",
    "# also, most tokenizers add special tokens such as <SOS>, <EOS>, <SEP>, <MASK>, etc.\n",
    "# padding done as at this level as well\n",
    "data = [[\"<SOS> \"+sentence+\" <EOS>\" for sentence in entry] for entry in data]\n",
    "\n",
    "MAX_SEQ_LEN = 10\n",
    "def pad(text):\n",
    "\twhile len(text.split(\" \")) < MAX_SEQ_LEN:\n",
    "\t\ttext += \" <PAD>\"\n",
    "\treturn text\n",
    "data = [[pad(sentence) for sentence in entry] for entry in data]\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10, 25]) torch.Size([3, 10, 25])\n"
     ]
    }
   ],
   "source": [
    "english_dict = {}\n",
    "idx = 1\n",
    "for entry in data:\n",
    "\ttokens = entry[0].split(\" \")\n",
    "\tfor token in tokens:\n",
    "\n",
    "\t\tif token not in english_dict:\n",
    "\t\t\tenglish_dict[token] = idx\n",
    "\t\t\tidx += 1\n",
    "\n",
    "idx = 1\n",
    "vietnamese_dict = {}\n",
    "idx = 0\n",
    "for entry in data:\n",
    "\ttokens = entry[1].split(\" \")\n",
    "\tfor token in tokens:\n",
    "\n",
    "\t\tif token not in vietnamese_dict:\n",
    "\t\t\tvietnamese_dict[token] = idx\n",
    "\t\t\tidx += 1\n",
    "\n",
    "EMBEDDING_SIZE = 25\n",
    "\n",
    "\n",
    "english = []\n",
    "for entry in data:\n",
    "\tsentence = entry[0]\n",
    "\ttokens = sentence.split(\" \")\n",
    "\n",
    "\tsequence = []\n",
    "\tfor token in tokens:\n",
    "\t\tohe = [0]*EMBEDDING_SIZE\n",
    "\t\tohe[english_dict[token]] = 1\n",
    "\t\tsequence.append(ohe)\n",
    "\n",
    "\tenglish.append(sequence)\n",
    "\n",
    "vietnamese = []\n",
    "for entry in data:\n",
    "\tsentence = entry[1]\n",
    "\ttokens = sentence.split(\" \")\n",
    "\n",
    "\tsequence = []\n",
    "\tfor token in tokens:\n",
    "\t\tohe = [0]*EMBEDDING_SIZE\n",
    "\t\tohe[vietnamese_dict[token]] = 1\n",
    "\t\tsequence.append(ohe)\n",
    "\n",
    "\tvietnamese.append(sequence)\n",
    "\n",
    "english = torch.Tensor(english)\n",
    "vietnamese = torch.Tensor(vietnamese)\n",
    "\n",
    "print(english.size(), vietnamese.size()) # batch size, sequence length, embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pytorch dataset and dataloader can be helpful\n",
    "class EN_VN_dataset(Dataset):\n",
    "\tdef __init__(self, lang1, lang2):\n",
    "\t\tself.lang1 = lang1\n",
    "\t\tself.lang2 = lang2\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.lang1.size(0)\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tsentence1 = self.lang1[index]\n",
    "\t\tsentence2 = self.lang2[index]\n",
    "\t\treturn sentence1, sentence2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\tdef __init__(self, input_size, hidden_size, num_layers=1, bidirectional=False, batch_first=True):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tself.rnn = nn.RNN(input_size=input_size,\n",
    "\t\t\t\t\thidden_size=hidden_size,\n",
    "\t\t\t\t\tnum_layers=num_layers,\n",
    "\t\t\t\t\tbidirectional=bidirectional,\n",
    "\t\t\t\t\tbatch_first=batch_first)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\toutput, hn = self.rnn(x)\n",
    "\t\treturn output, hn\n",
    "\n",
    "# check image on read me\n",
    "# if bidirectional = True\n",
    "# \taccording to torch LSTM doc, hn contains the final hidden states of forward and backward\n",
    "#\toutput contains the forward output and backward output at time step t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self, input_size, hidden_size, num_layers=1, bidirectional=False, batch_first=True):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\t\tself.rnn = nn.RNN(input_size=input_size,\n",
    "\t\t\t\t\thidden_size=hidden_size,\n",
    "\t\t\t\t\tnum_layers=num_layers,\n",
    "\t\t\t\t\tbidirectional=bidirectional,\n",
    "\t\t\t\t\tbatch_first=batch_first) # common to have decoder architecture similar to encoder\n",
    "\t\t\n",
    "\t\tself.linear = nn.Linear(hidden_size, EMBEDDING_SIZE) # classificatin head\n",
    "\t\tself.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\tdef forward(self, encoder_hidden):\n",
    "\t\tdecoder_hidden = encoder_hidden\n",
    "\n",
    "\t\t# we are going to unraveling the translation backwards\n",
    "\t\t# begin with the first input as <EOS>\n",
    "\t\tdecoder_input = []\n",
    "\t\tohe = [0]*EMBEDDING_SIZE\n",
    "\t\tohe[vietnamese_dict[\"<EOS>\"]] = 1\n",
    "\t\tdecoder_input.append([ohe]) # sequence length is one as we will step one at a time to change the input after each step\n",
    "\t\tdecoder_input = decoder_input*encoder_hidden.size(1)\n",
    "\t\tdecoder_input = torch.Tensor(decoder_input)\n",
    "\n",
    "\t\tdecoder_outputs = []\n",
    "\t\tdecoder_outputs.append(decoder_input.to(device))\n",
    "\t\tfor step in range(MAX_SEQ_LEN-1):\n",
    "\t\t\tdecoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "\t\t\tdecoder_outputs.append(decoder_output)\n",
    "\t\t\t# use the output from the decoder as the input now\n",
    "\t\t\t_, topidx = decoder_output.topk(1) # the index of the highest value\n",
    "\t\t\ttopidx = topidx.squeeze(-1)\n",
    "\t\t\tdecoder_input = [] # create single sequence ohe for batch\n",
    "\t\t\tfor entry in topidx:\n",
    "\t\t\t\tohe = [0]*EMBEDDING_SIZE \n",
    "\t\t\t\tohe[entry.item()] = 1\n",
    "\t\t\t\tdecoder_input.append([ohe])\n",
    "\t\t\tdecoder_input = torch.Tensor(decoder_input).detach()\n",
    "\t\t\n",
    "\t\tdecoder_outputs = torch.stack(decoder_outputs, dim=1).squeeze(2) # reshaping data\n",
    "\t\tdecoder_outputs = self.softmax(decoder_outputs)\n",
    "\t\treturn decoder_outputs, decoder_hidden\n",
    "\n",
    "\tdef forward_step(self, x, hn):\n",
    "\t\t# function to run though rnn one step\n",
    "\t\tx, hn = self.rnn(x.to(device), hn.to(device))\n",
    "\t\tx = self.linear(x)\n",
    "\t\treturn x, hn\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(EMBEDDING_SIZE, 128).to(device)\n",
    "decoder = Decoder(EMBEDDING_SIZE, 128).to(device)\n",
    "param_groups = [\n",
    "\t{'params': encoder.parameters(), 'lr': 0.0001},\n",
    "\t{'params': decoder.parameters(), 'lr': 0.0001}\n",
    "]\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(param_groups)\n",
    "\n",
    "dataset_loader = DataLoader(EN_VN_dataset(english, vietnamese), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.6800163984298706\n",
      " Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "\n",
    "\trunning_loss = 0.0\n",
    "\tfor i, data in enumerate(dataset_loader):\n",
    "\t\tinputs, labels = data\n",
    "\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\tencoder_output, encoder_hn = encoder(inputs)\n",
    "\t\tdecoder_output, decoder_hn = decoder(encoder_hn)\n",
    "\n",
    "\t\tloss = loss_fn(decoder_output, labels)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\n",
    "\tprint(f'\\r {running_loss/len(dataset_loader)}', end='', flush=True)\n",
    "\n",
    "print('\\n Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> we are going to church tomorrow <EOS> <PAD> <PAD> \n",
      "<EOS> ngay may minh di le <EOS> <PAD> <PAD> <PAD> "
     ]
    }
   ],
   "source": [
    "_, input_idxs = inputs.topk(1, dim=2)\n",
    "english_reverse = {}\n",
    "for k, v in english_dict.items():\n",
    "\tenglish_reverse[v] = k\n",
    "for idx in input_idxs.squeeze().tolist():\n",
    "\tprint(english_reverse[idx], end=' ')\n",
    "\n",
    "print()\n",
    "\n",
    "_, output_idxs = decoder_output.topk(1, dim=2)\n",
    "vietnamese_reverse = {}\n",
    "for k,v in vietnamese_dict.items():\n",
    "\tvietnamese_reverse[v] = k\n",
    "for idx in output_idxs.squeeze().tolist():\n",
    "\tprint(vietnamese_reverse[idx], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD> <PAD> <PAD> <EOS> le di minh may ngay <SOS> "
     ]
    }
   ],
   "source": [
    "_, labels_idxs = labels.topk(1, dim=2)\n",
    "labels_idxs = torch.flip(labels_idxs, dims=[1])\n",
    "for idx in labels_idxs.squeeze().tolist():\n",
    "\tprint(vietnamese_reverse[idx], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the word level, add <PAD> or any other special tokens\n",
    "make sure the data has the same order, structure as how you are planning to train the network\n",
    "- for example, decoder translates backwards so data should be backwards\n",
    "\n",
    "To help understand input to the decoder, remind yourself, how is the hidden and input x shaped/processed to be input to the RNN node. (can think as concatenated or seperate input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
